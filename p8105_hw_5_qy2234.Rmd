---
title: "p8105_hw5_qy2234"
author: "Michael Yan"
date: "11/5/2019"
output: github_document
---
```{r setup, include=FALSE}
## general setup
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
library(readxl)
library(patchwork)
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1
```{r}
## load in the given dataset and clean the names
set.seed(10)

iris_with_missing = 
  iris %>% 
  janitor::clean_names() %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(species = as.character(species))
```

```{r}
## create a funciton that fill in the missing values 
fill_missing = function(x) {
  if (is.numeric(x)) {
     x = replace_na(x, round(mean(x, na.rm = TRUE),1))
  }
  else if (is.character(x)) {
     x = replace_na(x, "virginica")
  }
  x
}

map(iris_with_missing, fill_missing) %>% 
  bind_cols()
```

# Problem 2
```{r}
## import data, create a read file function, combine files
file_list = list.files("./data")

read_file = function(x) {
  path = str_c("./data/",x)
  read_csv(path)
}

combined_df = 
  purrr::map(file_list,read_file) %>%
  bind_rows() %>% 
  janitor::clean_names() %>%
  mutate(file_list = file_list) %>% 
  separate(file_list, into = c("arm", "id"), sep = "_") %>% 
  mutate(arm = recode(arm, 
                      "con" = "control", 
                      "exp" = "experimental")) %>%
  mutate(id = str_remove(id,".csv")) %>% 
  select(id, arm, everything())

combined_df %>%
  knitr::kable()
```

```{r}
# make a spaghetti plot showing observations on each subject over time
combined_df %>% 
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "readings") %>% 
  ggplot(aes(x = week, y = readings, color = id, group = id)) +
  geom_line() +
  geom_point(aes(shape = arm)) +
  facet_grid(.~arm) +
  labs(title = "Spaghetti plots based on arm",
       x = "Week",
       y = "Readings")
```

* Based on the spaghetti plots made, we can see that there is not really an increase or a decrease for the control group. The observations for individuals fluctuate a lot but basiclly stay the same across the period of the experiment. There is not a significant enough trend. On the other hand, for the experimental group, although individual data still fluatuates a lot, there is a obvious increasing trend as the experiment proceed. In general, we see a higher average reading for the individuals in the experimental group compared to the ones in the control group.

# Problem 3
```{r}
## create a function that does simple linear regression (get the estimate and p-value)
set.seed(10)

sim_line_regression = function(n = 30, beta0 = 2, beta1 = 0) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 0, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, sqrt(50))
  )
  
  ls_fit = lm(y ~ x, data = sim_data) %>% 
    broom::tidy()
  tibble(
    beta1_hat = ls_fit[[2,2]],
    p_value = ls_fit[[2,5]]
  )
}
```

```{r}
## generate 10000 datasets from the model created
set.seed(10)

generate_10000_list =
  rerun(10000, sim_line_regression(beta1 = 0)) %>%
  bind_rows()

generate_10000_list
```

```{r}
## try again using beta1 = {1,2,3,4,5,6}
set.seed(10)

different_beta1_10000_lists = 
  tibble(beta1 = c(1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(beta1, ~rerun(10000, sim_line_regression(beta1 = .x)))) %>%
  unnest(output_lists) %>% 
  unnest()
```

```{r}
## make a plot showing the power of the test 
different_beta1_10000_lists %>% 
  group_by(beta1) %>%
  summarize(
    n = n(),
    power = sum(p_value < 0.05)/n) %>% 
  ggplot(aes(x = beta1, y = power)) +
  geom_smooth() +
  geom_point() +
    labs(title = "Association between effect size and power", 
         x = "True value of beta2", 
         y = "Power of the test")
```

* Based on the plot we made, we see a positive linear relationship between the true value of beta2 and the power of the test, in other words, as the true beta2 increases, the power of the test also increases.


```{r}
## comparing estimate of beta1 hat with the true value of beta1
population_average = 
different_beta1_10000_lists %>% 
  group_by(beta1) %>% 
  summarize(average_estimate_beta_1_pop = mean(beta1_hat)) 
 
sample_average = 
different_beta1_10000_lists %>% 
  filter(p_value < 0.05) %>% 
  group_by(beta1) %>% 
  summarize(average_estimate_beta_1_sam = mean(beta1_hat)) 

ggplot() + 
  geom_point(aes(x = beta1, y = average_estimate_beta_1_pop, color = "green"), 
             data = population_average) +
  geom_smooth(aes(x = beta1, y = average_estimate_beta_1_pop, color = "green"), 
              data = population_average) +
  geom_point(aes(x = beta1, y = average_estimate_beta_1_sam, color = "red"), 
             data = sample_average) +
  geom_smooth(aes(x = beta1, y = average_estimate_beta_1_sam, color = "red"), 
              data = sample_average) +
  scale_color_identity(breaks = c("green", "red"),
                       labels = c("True values", "Sample values"),
                       guide = "legend") +
  labs(title = "Comparing the estimate of beta1 hat with the true value of beta1", 
       x = "True value of beta1",
       y = "Average estimate of beta1 hat")
```

*The sample average of beta1 hat across tests for which the null is rejected is not approximately equal to the true value of beta1. The difference is larger when the true value of beta1 is small and smaller as the true value of beta1 gets larger.